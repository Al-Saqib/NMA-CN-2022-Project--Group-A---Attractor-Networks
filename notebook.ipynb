{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "OXnmTnJP3NSB",
      "metadata": {
        "id": "OXnmTnJP3NSB"
      },
      "source": [
        "# Hopfield Network Attractor Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebed637e-6801-43e7-ace9-0ec034e17c2e",
      "metadata": {
        "id": "ebed637e-6801-43e7-ace9-0ec034e17c2e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8bbbc6c-4b50-4e64-ab77-64fb936d0e31",
      "metadata": {
        "id": "b8bbbc6c-4b50-4e64-ab77-64fb936d0e31"
      },
      "source": [
        "# Basic classes, util functions, activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b71ce7-af07-4a1f-abc1-f10708ce88d3",
      "metadata": {
        "id": "42b71ce7-af07-4a1f-abc1-f10708ce88d3"
      },
      "outputs": [],
      "source": [
        "# Util functions\n",
        "def show_letter(pattern, ax = None):\n",
        "    if ax == None:\n",
        "      \n",
        "        f, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
        "        f.tight_layout()\n",
        "    side_len = int( pattern.size ** 0.5 + 0.5)\n",
        "    ax.imshow(pattern.reshape(side_len, side_len), cmap='bone_r')\n",
        "    ax.set_axis_off()\n",
        "\n",
        "def add_noise(x_, noise_level=.2):\n",
        "    noise = np.random.choice(\n",
        "        [1, -1], size=len(x_), p=[1-noise_level, noise_level])\n",
        "    return x_ * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40fd0341-c6dc-4a59-9dad-ebf2d09c576f",
      "metadata": {
        "id": "40fd0341-c6dc-4a59-9dad-ebf2d09c576f"
      },
      "outputs": [],
      "source": [
        "## Activation Functions\n",
        "\n",
        "def ReLU(ws):\n",
        "    data = [max(0,value) for value in ws]\n",
        "    return np.array(data, dtype=float)\n",
        "\n",
        "## for a single element\n",
        "def leakyRelu(ws):\n",
        "    if ws>0:\n",
        "        return ws\n",
        "    else:\n",
        "        return 0.01*ws\n",
        "\n",
        "## for a matrix\n",
        "def leaky_relu(ws):\n",
        "    alpha = 0.1\n",
        "    return np.maximum(alpha*ws, ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f30vlFh90E4",
      "metadata": {
        "id": "7f30vlFh90E4"
      },
      "source": [
        "### Update Rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xc7egZLH-HkZ",
      "metadata": {
        "id": "xc7egZLH-HkZ"
      },
      "outputs": [],
      "source": [
        "#storkey learning rule\n",
        "def storkey(self):\n",
        "    self.W = np.zeros([self.num_neurons, self.num_neurons])\n",
        "\n",
        "    for image_vector, _ in self.train_dataset:\n",
        "        self.W += np.outer(image_vector, image_vector) / self.num_neurons\n",
        "        net = np.dot(self.W, image_vector)\n",
        "\n",
        "        pre = np.outer(image_vector, net)\n",
        "        post = np.outer(net, image_vector)\n",
        "\n",
        "        self.W -= np.add(pre, post) / self.num_neurons\n",
        "    np.fill_diagonal(self.W, 0)\n",
        "\n",
        "    #Pseudo-Inverse Rule (implemented in C++)\n",
        "    # if (method == LearningMethod::PSEUDO_INVERSE)\n",
        "    # {\n",
        "    #     arma::mat w(N, N);\n",
        "    #     int m = samples.size(); // št. učnih vzorcev\n",
        "    #     for (int i = 0; i != N; i++)\n",
        "    #     {\n",
        "    #         for (int j = 0; j != N; j++)\n",
        "    #         {\n",
        "    #             w(i, j) = 0.0;\n",
        "    #             for (int v = 0; v != m; v++)\n",
        "    #             {\n",
        "    #                 for (int u = 0; u != m; u++)\n",
        "    #                 {\n",
        "    #                     w(i, j) += samples[v][i] * (1.0 / Q(u, v, samples)) * samples[u][j];\n",
        "    #                 }\n",
        "    #             }\n",
        "    #             w(i, j) = w(i, j) / N;\n",
        "    #         }\n",
        "    #     }\n",
        "    #     std::cout << w << endl;\n",
        "\n",
        "    #     // prekopiraj iz Armadillo mat objekta v 2D array\n",
        "    #     for (int i = 0; i != N; i++)\n",
        "    #     {\n",
        "    #         for (int j = 0; j != N; j++)\n",
        "    #         {\n",
        "    #             W[i][j] = w(i, j);\n",
        "    #         }\n",
        "    #     }\n",
        "    # }\n",
        "def extended_storkey_update(x, weights):\n",
        "    \"\"\"\n",
        "    Create an Op that performs a step of the Extended\n",
        "    Storkey Learning Rule.\n",
        "    Args:\n",
        "      sample: a 1-D x Tensor of dtype tf.bool.\n",
        "      weights: the weight matrix to update.\n",
        "    Returns:\n",
        "      An Op that updates the weights based on the sample.\n",
        "    \"\"\"\n",
        "    scale = 1 / int(weights.get_shape()[0])\n",
        "    numerics = 2*tf.cast(sample, weights.dtype) - 1\n",
        "    row_sample = tf.expand_dims(numerics, axis=0)\n",
        "    row_h = tf.matmul(row_sample, weights)\n",
        "\n",
        "    pos_term = (tf.matmul(tf.transpose(row_sample), row_sample) +\n",
        "                tf.matmul(tf.transpose(row_h), row_h))\n",
        "    neg_term = (tf.matmul(tf.transpose(row_sample), row_h) +\n",
        "                tf.matmul(tf.transpose(row_h), row_sample))\n",
        "    return tf.assign_add(weights, scale * (pos_term - neg_term))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "osE1t5YQ-H6Y",
      "metadata": {
        "id": "osE1t5YQ-H6Y"
      },
      "source": [
        "### The Main Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a67dfe-b923-4e43-93db-6991bb419ada",
      "metadata": {
        "id": "e7a67dfe-b923-4e43-93db-6991bb419ada"
      },
      "outputs": [],
      "source": [
        "class HopfieldNetwork(object):\n",
        "    def __init__(self, **kwargs):\n",
        "      pass\n",
        "\n",
        "    ## Training Step Options\n",
        "    # ----------------------\n",
        "\n",
        "    def TS_hebbian(self, training_list):\n",
        "        \"\"\"\n",
        "        Apply the hebbian training algorhithm \n",
        "        with training_list as the input\n",
        "        \"\"\"\n",
        "        m, n_units = np.shape(training_list)\n",
        "        self.m = m\n",
        "        self.n_units = n_units\n",
        "        self.training_list = training_list\n",
        "        self.weights = np.zeros((n_units, n_units))\n",
        "\n",
        "        # Memory lossiness warning\n",
        "        if n_units*0.14 < m:\n",
        "            print(\"The number of memory patterns to be stored is > 14%% \" +\n",
        "                \"of the model size. This may lead to problems.\" +\n",
        "                \"ref: https://doi.org/10.3389/fncom.2016.00144\")\n",
        "\n",
        "        # Hebbian rule\n",
        "        for x in training_list:\n",
        "            self.weights += np.outer(x, x) / m\n",
        "        self.weights[np.diag_indices(n_units)] = 0\n",
        "    \n",
        "    def TS_storkey(self, training_list):\n",
        "        # TODO: check if this works?\n",
        "        m, num_neurons = np.shape(training_list)\n",
        "        self.m = m\n",
        "        self.num_neurons = num_neurons\n",
        "        self.training_list = training_list\n",
        "        self.weights = np.zeros([self.num_neurons, self.num_neurons])\n",
        "\n",
        "        for image_vector in self.training_list:\n",
        "            self.weights += np.outer(image_vector, image_vector) / self.num_neurons\n",
        "            net = np.dot(self.weights, image_vector)\n",
        "\n",
        "            pre = np.outer(image_vector, net)\n",
        "            post = np.outer(net, image_vector)\n",
        "\n",
        "            self.weights -= np.add(pre, post) / self.num_neurons\n",
        "        np.fill_diagonal(self.weights, 0)\n",
        "\n",
        "\n",
        "    # Inference Step Options\n",
        "    # ----------------------\n",
        "\n",
        "    def IS_tanh_threshold(self, X, N, gradient=1000, threshold=1):\n",
        "        \"\"\"\n",
        "        Run the inference step N times starting with the input X0\n",
        "        The activation function is tanh(a*x + b)\n",
        "        Set gradient to 1 for normal tanh\n",
        "        \"\"\"\n",
        "        # set up empty history\n",
        "        Xs = np.zeros((N, len(X)))\n",
        "\n",
        "        for i in range(N):\n",
        "            # weighted sums\n",
        "            ws = np.dot(X, self.weights)\n",
        "\n",
        "            # activation function\n",
        "            X = np.tanh(gradient * (ws - threshold))\n",
        "\n",
        "            # check if there's change from previous entry\n",
        "            if i > 0:\n",
        "                if self._calculate_error(Xs[i-1], X) == 0:\n",
        "                    Xs = Xs[:i]\n",
        "                    # print(f\"quit after {i} steps: steady state reached\")\n",
        "                    break\n",
        "\n",
        "            # add entry to state history\n",
        "            Xs[i] = X.copy()\n",
        "\n",
        "        self.inference_history = Xs\n",
        "        return Xs\n",
        "    \n",
        "\n",
        "    ## Evaluation functions\n",
        "    # ---------------------\n",
        "\n",
        "    def energy(self):\n",
        "        \"\"\"sum of values >= 0 over the inference history\"\"\"\n",
        "        return np.sum([self.inference_history >= 0])\n",
        "    \n",
        "    def time(self):\n",
        "        return len(self.inference_history)\n",
        "\n",
        "    def perf(self, test):\n",
        "        \"\"\"difference between chosen input\"\"\"\n",
        "        return self._calculate_error(test, self.inference_history[-1])\n",
        "\n",
        "    def best_fit(self, y_hat):\n",
        "        X_predict = self.inference_history[-1]\n",
        "        score = self._validate(X_predict, y_hat)\n",
        "        # TODO:\n",
        "        return score\n",
        "\n",
        "    def _validate(self, X_predict, y_hat):\n",
        "        '''\n",
        "        Return value: (is_correct, mse_to_target)\n",
        "            is_correct: 0 or 1\n",
        "            mse_target: Scalar\n",
        "        '''\n",
        "        # find the most similar picture in the dataset\n",
        "        min_error = 1e10\n",
        "        min_error_idx = -1\n",
        "        for y_idx in range(self.m):\n",
        "            # print(X_predict, self.training_list[y_idx])\n",
        "            this_error = self._calculate_error(X_predict, self.training_list[y_idx])\n",
        "            # print(f\"idx={y_idx}, error={this_error}\")\n",
        "            if this_error < min_error:\n",
        "                min_error = this_error\n",
        "                min_error_idx = y_idx\n",
        "        # print(min_error, min_error_idx)\n",
        "        return (min_error_idx == y_hat), self._calculate_error(X_predict, self.training_list[y_hat])\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_error(x1, x2):\n",
        "        return np.sum(np.abs(x1 - x2))\n",
        "\n",
        "\n",
        "class PerformanceMetric(object):\n",
        "    def __init__(self):\n",
        "        self.time = []\n",
        "        self.energy = []\n",
        "        self.is_correct = []\n",
        "        self.error = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203ee584-1de6-4b8f-bc13-ef4e15834143",
      "metadata": {
        "id": "203ee584-1de6-4b8f-bc13-ef4e15834143"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a376f438-6832-4e22-9a74-a31cfa76d21f",
      "metadata": {
        "id": "a376f438-6832-4e22-9a74-a31cfa76d21f"
      },
      "source": [
        "### Dataset 2 (Demo, whole letters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55bc7e1c-989b-46af-aa37-90ac8b2be088",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55bc7e1c-989b-46af-aa37-90ac8b2be088",
        "outputId": "38e4fb5f-3a1c-4bd5-b6a0-6cc1f13c39d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import imageio as iio\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# read an image\n",
        "img = iio.imread(\"/content/drive/MyDrive/Attractor Network Project/Images/pixel-arial-11-font-character-map.png\")\n",
        "# print(img.shape)\n",
        "plt.imshow(img)\n",
        "\n",
        "def generate_all_letter_dataset(new_size = 64):\n",
        "    pixel_i = 115\n",
        "    pixel_j = 110\n",
        "    all_letter_final = np.empty((9 * 9, new_size, new_size), dtype=\"int8\")\n",
        "    for i in range(9):\n",
        "        for j in range(9):\n",
        "            # print(i, j)\n",
        "            i_offset = 0\n",
        "            if i >= 3:\n",
        "                i_offset = 40\n",
        "            if i >= 6:\n",
        "                i_offset = 80\n",
        "\n",
        "            subimg = img[i_offset + pixel_i * i : i_offset +pixel_i * (i+1),\n",
        "                         pixel_j * j : pixel_j * (j+1),\n",
        "                         :4].astype(\"uint8\")\n",
        "            new_img = Image.fromarray(subimg)\n",
        "\n",
        "            new_img = new_img.resize((new_size, new_size))\n",
        "            new_arr = np.asarray(new_img)\n",
        "            # print(new_arr[20:30, 20:30, 3])\n",
        "            new_arr = (new_arr[:, :, 3] > 100) * 2 - 1\n",
        "            # all_letter_final[i * 9 + j] = new_arr[:, :, 0]\n",
        "            all_letter_final[i*9+j] = new_arr[:]\n",
        "    all_letter_final = all_letter_final.reshape(all_letter_final.shape[0], -1)\n",
        "    return all_letter_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-GDXn5o7gllc",
      "metadata": {
        "id": "-GDXn5o7gllc"
      },
      "outputs": [],
      "source": [
        "# Calculate all letters up to resolution 100\n",
        "\n",
        "LETTERS_RESOLUTION = [generate_all_letter_dataset(new_size=1)]\n",
        "for i in range(0, 100):\n",
        "    LETTERS_RESOLUTION.append(generate_all_letter_dataset(new_size = i+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RQKMWLtleE6D",
      "metadata": {
        "id": "RQKMWLtleE6D"
      },
      "outputs": [],
      "source": [
        "# # Set the letter resolution\n",
        "\n",
        "# all_letter_final = generate_all_letter_dataset(new_size = 6)\n",
        "# # print(all_letter_final)\n",
        "# print(all_letter_final.shape)\n",
        "# for i in range(9):\n",
        "#     for j in range(9):\n",
        "#         arr = all_letter_final[9 * i + j]\n",
        "#         ax = plt.subplot(9, 9, i * 9 + j + 1)\n",
        "#         show_letter(arr, ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZdHa7xfNdLpJ",
      "metadata": {
        "id": "ZdHa7xfNdLpJ"
      },
      "outputs": [],
      "source": [
        "LETTERS_RESOLUTION[10].shape\n",
        "all_letter_final = LETTERS_RESOLUTION[10]\n",
        "for i in range(9):\n",
        "    for j in range(9):\n",
        "        arr = all_letter_final[9 * i + j]\n",
        "        ax = plt.subplot(9, 9, i * 9 + j + 1)\n",
        "        show_letter(arr, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cRWUtYa3A2I5",
      "metadata": {
        "id": "cRWUtYa3A2I5"
      },
      "source": [
        "### Dataset 3 (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sTYYJtyJA5ww",
      "metadata": {
        "id": "sTYYJtyJA5ww"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# GET mnist data\n",
        "mnist = fetch_openml(name='mnist_784', as_frame = False)\n",
        "mnist_X = mnist.data\n",
        "mnist_X.shape\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "show_first_n = 10\n",
        "for i in range(show_first_n):\n",
        "  plt.subplot(1, show_first_n, i +1)\n",
        "  plt.imshow(mnist_X[i].reshape(28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DZGxbf2B3WPT",
      "metadata": {
        "id": "DZGxbf2B3WPT"
      },
      "source": [
        "### Dataset 4(Demyan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AIP0J_cA4JZt",
      "metadata": {
        "id": "AIP0J_cA4JZt"
      },
      "outputs": [],
      "source": [
        "import imageio as iio\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G6eRiQcl4bK_",
      "metadata": {
        "id": "G6eRiQcl4bK_"
      },
      "outputs": [],
      "source": [
        "# all_images_dem = np.empty((6, 200 * 200))\n",
        "all_images_dem = np.empty((6, 50 * 50))\n",
        "for i in range(6):\n",
        "  img = iio.imread(f\"/content/drive/MyDrive/Attractor Network Project/Images/Hand Drawn/dem_{i+1}.jpg\")\n",
        "  print(img.shape)\n",
        "  new_img = img.copy().astype(\"int\")\n",
        "  new_img[new_img < 100] = -1\n",
        "  new_img[new_img >= 100] = 1\n",
        "  new_img = new_img[::4, ::4]\n",
        "  all_images_dem[i] = new_img.reshape(-1).copy()\n",
        "\n",
        "# img = iio.imread(f\"/content/drive/MyDrive/Attractor Network Project/Images/Hand Drawn/dem_1.jpg\")\n",
        "# all_images_dem.shape\n",
        "# plt.hist(img.reshape(-1))\n",
        "# img_set = set(tuple(img.reshape(-1).tolist()))\n",
        "# print(img_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CBzQ1Bag4tKW",
      "metadata": {
        "id": "CBzQ1Bag4tKW"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 6)\n",
        "for i in range(6):\n",
        "  show_letter(all_images_dem[i], axs[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf0c67a-7496-4de1-9f88-8823f168b749",
      "metadata": {
        "id": "fbf0c67a-7496-4de1-9f88-8823f168b749"
      },
      "source": [
        "# Training and validation process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f74d9f32-d8c3-4e8a-88a1-58fae0a32eba",
      "metadata": {
        "id": "f74d9f32-d8c3-4e8a-88a1-58fae0a32eba"
      },
      "source": [
        "### Demo for dataset 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "likCT4U3nxIF",
      "metadata": {
        "id": "likCT4U3nxIF"
      },
      "outputs": [],
      "source": [
        "# Setup and train\n",
        "np.random.seed(0)\n",
        "letter_index = [0,1 ,2,3]\n",
        "letter_res = 4\n",
        "\n",
        "all_letter_dataset = LETTERS_RESOLUTION[letter_res][letter_index]\n",
        "\n",
        "all_letter_dataset = np.random.randint(0, 1, size = )\n",
        "n_image, image_size = all_letter_dataset.shape\n",
        "print(f\"Num of images = {n_image}\")\n",
        "\n",
        "hop_net = HopfieldNetwork()\n",
        "hop_net.TS_hebbian(all_letter_dataset)\n",
        "\n",
        "# alphabet preview\n",
        "letter_preview = LETTERS_RESOLUTION[letter_res]\n",
        "print(letter_preview.shape)\n",
        "for i in range(9):\n",
        "    for j in range(9):\n",
        "        arr = letter_preview[9 * i + j]\n",
        "        ax = plt.subplot(9, 9, i * 9 + j + 1)\n",
        "        show_letter(arr, ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pYZkGT2_zZ6S",
      "metadata": {
        "id": "pYZkGT2_zZ6S"
      },
      "outputs": [],
      "source": [
        "for (i, j) in [(0, 1), (0, 2), (0, 3)]:\n",
        "  print(i, j, np.corrcoef(all_letter_dataset[i], all_letter_dataset[j])[0][1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9e0641-ab67-4c80-a271-7745f249c299",
      "metadata": {
        "id": "7a9e0641-ab67-4c80-a271-7745f249c299"
      },
      "outputs": [],
      "source": [
        "# TODO: try to implement this thing where it shows the letters we're using?\n",
        "# I duno how to do it in one plot...\n",
        "\n",
        "# Show chosen letters\n",
        "# for i, ltr in enumerate(all_letter_dataset):\n",
        "#     ax = plt.subplot(2, graph_len, i+1)\n",
        "#     show_letter(ltr, ax)\n",
        "\n",
        "# Run the inference\n",
        "\n",
        "noise_level = .0\n",
        "# n_test_samples = 1000\n",
        "n_test_samples = 10\n",
        "n_iter = 100\n",
        "show_last_n_letters = 3\n",
        "for i in range(n_test_samples):\n",
        "    # pick random image\n",
        "    letter_idx = np.random.randint(0, n_image)\n",
        "\n",
        "    # set custom letter index\n",
        "    # letter_idx = 1\n",
        "\n",
        "    # add noise\n",
        "    x_test = all_letter_dataset[letter_idx].copy()\n",
        "    x_test = add_noise(x_test, noise_level=noise_level)\n",
        "    Xs = hop_net.IS_tanh_threshold(x_test, N=n_iter)\n",
        "\n",
        "    # plot input\n",
        "    ax = plt.subplot(n_test_samples, 2, i*2+1)\n",
        "    show_letter(x_test, ax)\n",
        "    ax = plt.subplot(n_test_samples, 2, i*2+2)\n",
        "    show_letter(Xs[-1], ax)\n",
        "\n",
        "    # # plot last n entries\n",
        "    # for i, X in enumerate(Xs[-3:]):\n",
        "    #     ax = plt.subplot(1, show_last_n_letters + 2, i+3)\n",
        "    #     show_letter(X, ax)\n",
        "        \n",
        "    # TODO: evaluate the function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WFtuBRFXFJiS",
      "metadata": {
        "id": "WFtuBRFXFJiS"
      },
      "source": [
        "### A demo for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T49XaJ3NFIMC",
      "metadata": {
        "id": "T49XaJ3NFIMC"
      },
      "outputs": [],
      "source": [
        "# np.random.seed(0)\n",
        "\n",
        "# all_letter_dataset = generate_all_letter_dataset(new_size = 150)\n",
        "# all_letter_dataset = all_letter_dataset[[10, 20, 30, 5]]\n",
        "# # all_letter_dataset = all_letter_dataset[::-1]\n",
        "# n_image, image_size = all_letter_dataset.shape\n",
        "# print(f\"Num of images = {n_image}\")\n",
        "\n",
        "# hop_net = HopfieldNetwork()\n",
        "# hop_net.TS_hebbian(all_letter_dataset)\n",
        "\n",
        "# noise_level = .20\n",
        "# # n_test_samples = 1000\n",
        "# n_test_samples = 1\n",
        "# n_iter = 1000\n",
        "# for i in range(n_test_samples):\n",
        "#     # add noise\n",
        "#     # random_idx = np.random.randint(0, n_image - 1)\n",
        "#     random_idx = 3\n",
        "#     print(random_idx)\n",
        "#     x_test = all_letter_dataset[random_idx].copy()\n",
        "#     x_test = add_noise(x_test, noise_level=noise_level)\n",
        "#     is_correct, error = hop_net.score(x_test, random_idx)\n",
        "#     print(is_correct, error)\n",
        "#     if i == 0:\n",
        "#       fig, axs = plt.subplots(1, 6)\n",
        "#       show_letter(all_letter_dataset[0], axs[0])\n",
        "#       show_letter(all_letter_dataset[1], axs[1])\n",
        "#       show_letter(all_letter_dataset[2], axs[2])\n",
        "#       show_letter(all_letter_dataset[3], axs[3])\n",
        "#       show_letter(x_test, axs[4])\n",
        "#       show_letter(hop_net.IS_tanh_threshold(x_test)[-1], axs[5])\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YiRr1safhDYV",
      "metadata": {
        "id": "YiRr1safhDYV"
      },
      "source": [
        "### Demo for dataset 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DZBiqLtTlaGU",
      "metadata": {
        "id": "DZBiqLtTlaGU"
      },
      "outputs": [],
      "source": [
        "mnist_X[0].min(),mnist_X[0].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6K6q4oRHlqPV",
      "metadata": {
        "id": "6K6q4oRHlqPV"
      },
      "outputs": [],
      "source": [
        "n_images = 10\n",
        "all_letter_dataset = mnist_X[:n_images].copy()\n",
        "all_letter_dataset[all_letter_dataset < 10] = -1\n",
        "all_letter_dataset[all_letter_dataset >= 10] = 1\n",
        "\n",
        "for i in range(n_images):\n",
        "  ax = plt.subplot(1, n_images, i+1)\n",
        "  show_letter(all_letter_dataset[i], ax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZE4tlLE9hFP_",
      "metadata": {
        "id": "ZE4tlLE9hFP_"
      },
      "outputs": [],
      "source": [
        "hop_net = HopfieldNetwork()\n",
        "hop_net.TS_hebbian(all_letter_dataset)\n",
        "\n",
        "noise_level = .20\n",
        "# n_test_samples = 1000\n",
        "n_test_samples = 50\n",
        "n_iter = 1000\n",
        "for i in range(n_test_samples):\n",
        "    # add noise\n",
        "    random_idx = np.random.randint(0, n_images)\n",
        "    # random_idx = 3\n",
        "    x_test = all_letter_dataset[random_idx].copy()\n",
        "    x_test = add_noise(x_test, noise_level=noise_level)\n",
        "    Xs = hop_net.IS_tanh_threshold(x_test, 10)\n",
        "    is_correct, error = hop_net._validate(x_test, random_idx)\n",
        "    print(random_idx, is_correct, error)\n",
        "\n",
        "    ax = plt.subplot(n_test_samples, 2, i * 2 +1)\n",
        "    show_letter(x_test, ax)\n",
        "    ax = plt.subplot(n_test_samples, 2, i * 2 +2)\n",
        "    show_letter(Xs[-1], ax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hpSYFn-q73pz",
      "metadata": {
        "id": "hpSYFn-q73pz"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(hop_net.weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WLO1cCk464xW",
      "metadata": {
        "id": "WLO1cCk464xW"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(hop_net.weights[:100, :100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aV_lValM7Tsp",
      "metadata": {
        "id": "aV_lValM7Tsp"
      },
      "source": [
        "### Demo for Dataset 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "foXJxhl-7WWW",
      "metadata": {
        "id": "foXJxhl-7WWW"
      },
      "outputs": [],
      "source": [
        "# all_images_dem\n",
        "hop_net = HopfieldNetwork()\n",
        "hop_net.TS_storkey(all_images_dem)\n",
        "\n",
        "noise_level = .0\n",
        "# n_test_samples = 1000\n",
        "n_test_samples = 1\n",
        "n_iter = 1000\n",
        "n_images = len(all_images_dem)\n",
        "for i in range(n_test_samples):\n",
        "    # add noise\n",
        "    # random_idx = np.random.randint(0, n_images)\n",
        "    random_idx = 2\n",
        "    x_test = all_images_dem[random_idx].copy()\n",
        "    x_test = add_noise(x_test, noise_level=noise_level)\n",
        "    Xs = hop_net.IS_tanh_threshold(x_test, 10)\n",
        "    is_correct, error = hop_net._validate(x_test, random_idx)\n",
        "    print(random_idx, is_correct, error)\n",
        "\n",
        "    ax = plt.subplot(n_test_samples, 2, i * 2 + 1)\n",
        "    show_letter(x_test, ax)\n",
        "    ax = plt.subplot(n_test_samples, 2, i * 2 + 2)\n",
        "    show_letter(Xs[-1], ax)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sLqtgJGhdzKg",
      "metadata": {
        "id": "sLqtgJGhdzKg"
      },
      "source": [
        "# Let's do experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zkSb71ird2F2",
      "metadata": {
        "id": "zkSb71ird2F2"
      },
      "source": [
        "### Exp 1: only changing number of neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l3NukCefd6C9",
      "metadata": {
        "id": "l3NukCefd6C9"
      },
      "outputs": [],
      "source": [
        "res = 16\n",
        "ninput = 4\n",
        "noise_level = .20\n",
        "# n_test_samples = 1000\n",
        "n_test_samples = 10\n",
        "n_iter = 4\n",
        "show_last_n_letters = 3\n",
        "np.random.seed(4)\n",
        "\n",
        "def do_test(res, dataset, noise_level, n_test_samples, show_last_n_letters):\n",
        "  ninput = dataset.shape[0]\n",
        "  hop_net = HopfieldNetwork()\n",
        "  hop_net.TS_hebbian(dataset)\n",
        "  # for i in range(ninput):\n",
        "  #   ax = plt.subplot(1, ninput, i+1)\n",
        "  #   show_letter(dataset[i], ax)\n",
        "  pm = PerformanceMetric()\n",
        "\n",
        "  for i in range(n_test_samples):\n",
        "    # pick random image\n",
        "    letter_idx = np.random.randint(0, ninput - 1)\n",
        "    # set custom letter index\n",
        "    # letter_idx = 1\n",
        "\n",
        "    # add noise\n",
        "    x_test_raw = all_letter_dataset[letter_idx].copy()\n",
        "    x_test = add_noise(x_test_raw, noise_level=noise_level)\n",
        "    Xs = hop_net.IS_tanh_threshold(x_test, N=n_iter)\n",
        "    \n",
        "    is_correct, error = hop_net._validate(Xs[-1], letter_idx)\n",
        "    # print(hop_net.time())\n",
        "    # print(hop_net.energy())\n",
        "    # print(is_correct, error)\n",
        "    pm.time.append(hop_net.time())\n",
        "    pm.energy.append(hop_net.energy())\n",
        "    pm.is_correct.append(is_correct)\n",
        "    pm.error.append(error)\n",
        "  return pm\n",
        "    \n",
        "    # print(hop_net.perf())\n",
        "    # # plot input\n",
        "    # ax = plt.subplot(1, show_last_n_letters + 1, 1)\n",
        "    # show_letter(x_test_raw, ax)\n",
        "\n",
        "    # # plot last n entries\n",
        "    # for i, X in enumerate(Xs[-3:]):\n",
        "    #     ax = plt.subplot(1, show_last_n_letters + 1, i+2)\n",
        "    #     show_letter(X, ax)\n",
        "\n",
        "# this_letter_dataset = LETTERS_RESOLUTION[letter_res][range(0, ninput)]\n",
        "all_letter_dataset = mnist_X[:ninput]\n",
        "pm = do_test(res, all_letter_dataset, noise_level, n_test_samples, show_last_n_letters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wtBscoS5pnXl",
      "metadata": {
        "id": "wtBscoS5pnXl"
      },
      "outputs": [],
      "source": [
        "# pms = []\n",
        "# min_res, max_res = 1, 10\n",
        "# n_test_samples = 30\n",
        "# neurons = [x**2 for x in range(min_res, max_res)]\n",
        "# np.random.seed(10)\n",
        "# for res in range(min_res, max_res):\n",
        "#   pm = do_test(res, ninput, noise_level, n_test_samples, show_last_n_letters)\n",
        "#   pms.append(pm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mGePiq5diWuU",
      "metadata": {
        "id": "mGePiq5diWuU"
      },
      "source": [
        "# Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SKASe5G-f5_w",
      "metadata": {
        "id": "SKASe5G-f5_w"
      },
      "outputs": [],
      "source": [
        "pms = [pm]\n",
        "neurons = [28 * 28]\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "# 1\n",
        "ax_num_acc = axs[0][0]\n",
        "ax_num_acc.plot(neurons, [np.mean(pm.is_correct) for pm in pms])\n",
        "ax_num_acc.set_xlabel('#Neurons')\n",
        "ax_num_acc.set_ylabel(\"Accuracy\")\n",
        "ax_num_acc.set_xticks(neurons)\n",
        "ax_num_acc.set_title(f\"#Neuron v.s. Accuracy (#trial = {n_test_samples}, #input_sample={ninput})\")\n",
        "\n",
        "\n",
        "# ax timestep accuracy\n",
        "ax_time_acc = axs[0][1]\n",
        "ax_time_acc.scatter([np.mean(pm.time) for pm in pms], [np.mean(pm.is_correct) for pm in pms])\n",
        "ax_time_acc.set_xlabel(f\"Mean timestamp on {n_test_samples} trials\")\n",
        "ax_time_acc.set_ylabel(\"Accuracy\")\n",
        "ax_time_acc.set_title(\"Mean timestamp v.s. Accuracy\")\n",
        "\n",
        "# ax energy accuracy\n",
        "ax_energy_acc = axs[1][0]\n",
        "ax_energy_acc.scatter([np.mean(pm.energy) for pm in pms], [np.mean(pm.is_correct) for pm in pms])\n",
        "ax_energy_acc.set_xlabel(f\"Mean energy on {n_test_samples} trials\")\n",
        "ax_energy_acc.set_ylabel(\"Accuracy\")\n",
        "ax_energy_acc.set_title(\"Mean energy v.s. Accuracy\")\n",
        "\n",
        "# ax energy accuracy\n",
        "ax_energy_time = axs[1][1]\n",
        "ax_energy_time.scatter([np.mean(pm.energy) for pm in pms], [np.mean(pm.time) for pm in pms])\n",
        "ax_energy_time.set_xlabel(\"Mean energy\")\n",
        "ax_energy_time.set_ylabel(\"Mean Timestamp\")\n",
        "ax_energy_time.set_title(\"Mean energy v.s. Mean Timestamp\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WscgaV2wuyKl",
      "metadata": {
        "id": "WscgaV2wuyKl"
      },
      "source": [
        "### Exp2: Change the learning rule / activation functin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sxMOqQeHu3ey",
      "metadata": {
        "id": "sxMOqQeHu3ey"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f1b6c3c4-d623-475e-9f8b-228f5b835ed5",
      "metadata": {
        "id": "f1b6c3c4-d623-475e-9f8b-228f5b835ed5"
      },
      "source": [
        "# Plotting Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591cd4bd-5398-40b1-b855-058378c03b2e",
      "metadata": {
        "id": "591cd4bd-5398-40b1-b855-058378c03b2e"
      },
      "outputs": [],
      "source": [
        "# neurons = [25, 36, 49, 64, 81]\n",
        "# accuracy = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "# time = [10, 50, 30, 10, 20]\n",
        "\n",
        "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
        "# ax1.plot(neurons, accuracy)\n",
        "# ax1.set_xlabel(\"#Neurons\")\n",
        "# ax1.set_ylabel(\"Accuracy\")\n",
        "# ax1.set_ylim(0, 1)\n",
        "\n",
        "# ax2.plot(neurons, time)\n",
        "# ax2.set_xlabel(\"#Neurons\")\n",
        "# ax2.set_ylabel(\"Time\")\n",
        "\n",
        "# ax3.plot(time, accuracy)\n",
        "# ax3.set_xlabel(\"Time\")\n",
        "# ax3.set_ylabel(\"Accuracy\")\n",
        "# ax3.set_ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tLTk84sA-2G4",
      "metadata": {
        "id": "tLTk84sA-2G4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "a376f438-6832-4e22-9a74-a31cfa76d21f",
        "cRWUtYa3A2I5",
        "f74d9f32-d8c3-4e8a-88a1-58fae0a32eba",
        "WFtuBRFXFJiS",
        "YiRr1safhDYV",
        "sLqtgJGhdzKg",
        "f1b6c3c4-d623-475e-9f8b-228f5b835ed5"
      ],
      "name": "main_bjm_addplot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "1d1026617dd78001c52a204c5cd2cde2efafd1a3b285591fb0f82885a58dba3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
